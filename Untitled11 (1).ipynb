{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFd2edYoRqbK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "def parse_json(algoparams_from_ui.json.rtf):\n",
        "    with open(algoparams_from_ui.json.rtf, 'r') as file:\n",
        "        config = json.load(file)\n",
        "    return config\n",
        "\n",
        "def load_data(iris.csv, target_column):\n",
        "    df = pd.read_csv(iris.csv)\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "    return X, y\n",
        "\n",
        "def build_pipeline(config):\n",
        "    steps = []\n",
        "\n",
        "    # Step 1: Imputation\n",
        "    imputer_strategy = config.get('imputation', {}).get('strategy', 'mean')\n",
        "    steps.append(('imputer', SimpleImputer(strategy=imputer_strategy)))\n",
        "\n",
        "    # Step 2: Feature Reduction\n",
        "    reduction_method = config.get('feature_reduction', {}).get('method', 'none')\n",
        "    if reduction_method == 'PCA':\n",
        "        n_components = config['feature_reduction'].get('n_components', 2)\n",
        "        steps.append(('reduction', PCA(n_components=n_components)))\n",
        "    elif reduction_method == 'Corr with Target':\n",
        "        steps.append(('reduction', SelectKBest(score_func=f_regression, k='all')))\n",
        "    elif reduction_method == 'Tree-based':\n",
        "        steps.append(('reduction', RandomForestRegressor(n_estimators=10)))\n",
        "\n",
        "    # Step 3: Model\n",
        "    models_config = config.get('models', [])\n",
        "    pipelines = []\n",
        "    for model_cfg in models_config:\n",
        "        if model_cfg.get('is_selected', False):\n",
        "            model_type = model_cfg['type']\n",
        "            if model_type == 'LinearRegression':\n",
        "                model = LinearRegression()\n",
        "            elif model_type == 'RandomForest':\n",
        "                model = RandomForestRegressor()\n",
        "            elif model_type == 'DecisionTree':\n",
        "                model = DecisionTreeRegressor()\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported model type: {model_type}\")\n",
        "\n",
        "            steps.append(('model', model))\n",
        "            pipelines.append(Pipeline(steps))\n",
        "\n",
        "    return pipelines\n",
        "\n",
        "def run_pipeline(X, y, pipeline, hyperparameters):\n",
        "    grid_search = GridSearchCV(pipeline, hyperparameters, cv=5, scoring='r2', verbose=1)\n",
        "    grid_search.fit(X, y)\n",
        "    return grid_search\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    print(f\"Mean Squared Error: {mse}\")\n",
        "    print(f\"Mean Absolute Error: {mae}\")\n",
        "    print(f\"R2 Score: {r2}\")\n",
        "\n",
        "def main(json_path, csv_path):\n",
        "    # Parse JSON and load data\n",
        "    config = parse_json(json_path)\n",
        "    target_column = config['target']['target']\n",
        "    X, y = load_data(csv_path, target_column)\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Build pipelines\n",
        "    pipelines = build_pipeline(config)\n",
        "\n",
        "    # Run pipelines and evaluate\n",
        "    for pipeline in pipelines:\n",
        "        hyperparameters = config.get('hyperparameters', {})\n",
        "        best_model = run_pipeline(X_train, y_train, pipeline, hyperparameters)\n",
        "        evaluate_model(best_model.best_estimator_, X_test, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Update paths to your JSON file and dataset\n",
        "    json_file = 'algoparams_from_ui.json.rtf'\n",
        "    csv_file_path = 'iris.csv'\n",
        "    main(json_file, csv_file_path)\n"
      ]
    }
  ]
}